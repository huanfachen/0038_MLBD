{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "racial-salvation",
      "metadata": {
        "id": "racial-salvation"
      },
      "source": [
        "# Lecture 25: Data Version Control II"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empirical-video",
      "metadata": {
        "id": "empirical-video"
      },
      "source": [
        "![](https://www.tensorflow.org/images/colab_logo_32px.png)\n",
        "[Run in colab](https://colab.research.google.com/drive/106YjL7FM57HsYQMU3X5DZM2eEt_Nyewd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "miniature-prediction",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-04T12:17:48.678473Z",
          "iopub.status.busy": "2022-02-04T12:17:48.678007Z",
          "iopub.status.idle": "2022-02-04T12:17:48.686792Z",
          "shell.execute_reply": "2022-02-04T12:17:48.686332Z"
        },
        "id": "miniature-prediction",
        "outputId": "90ffc09f-7ab9-4506-aabb-9cdedb627739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last executed: 2022-02-04 12:17:48\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "satisfied-poetry",
      "metadata": {
        "id": "satisfied-poetry"
      },
      "source": [
        "This lecture is part of a series on [Data Version control (DVC)](https://dvc.org), a way of systematically keeping track of different versions of models and datasets.\n",
        "\n",
        "This second lecture in the series will cover\n",
        "- including files from external sources\n",
        "- creating and rerunning pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alike-mailing",
      "metadata": {
        "id": "alike-mailing"
      },
      "source": [
        "## Adding external files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "living-protest",
      "metadata": {
        "id": "living-protest"
      },
      "source": [
        "We have seen how we can track our own files with **dvc add**. But what if we want to include data or other files that are already available?\n",
        "\n",
        "DVC offers different options:\n",
        "- **dvc get** to download a file from a DVC or git repository\n",
        "- **dvc import** to reuse a file from a DVC or git repository\n",
        "- **dvc get-url** and **dvc import-url** to copy or reuse a file from general remote storage, e.g. S3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ideal-honor",
      "metadata": {
        "id": "ideal-honor"
      },
      "source": [
        "The [`get`](https://dvc.org/doc/command-reference/get) and [`import`](https://dvc.org/doc/command-reference/import) commands are similar. The difference is that the latter also links to the original file and tracks its history. Therefore, if an update is made to the original repository later on, we can get the changes to our copy (see [`dvc update`](https://dvc.org/doc/command-reference/update)).\n",
        "\n",
        "The [`get-url`](https://dvc.org/doc/command-reference/get-url) and [`import-url`](https://dvc.org/doc/command-reference/import-url) variants are useful when the original data is not already in a repository. They can handle different storage protocols and providers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "collectible-exemption",
      "metadata": {
        "id": "collectible-exemption"
      },
      "source": [
        "## Creating pipelines\n",
        "\n",
        "One of the core benefits of using a data-focused version control system is that we can structure our work around data flows, not individual files.\n",
        "\n",
        "With DVC, we can\n",
        "- specify each stage of a pipeline\n",
        "- infer the connections between them\n",
        "- run a whole pipeline or only the parts required"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bored-creek",
      "metadata": {
        "id": "bored-creek"
      },
      "source": [
        "Let's consider this example:\n",
        "![Image of a data pipeline with a set of samples undergoing PCA and logistic regression. Each step is represented as a box with arrows linking them.](https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture25_Images/pipeline_example.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "selective-tucson",
      "metadata": {
        "id": "selective-tucson"
      },
      "source": [
        "We start with a set of a labelled samples contained in a file `samples.csv`.\n",
        "\n",
        "The first step is to run the code in `reduce_dim.py`. This reads the inputs and performs PCA on them to reduce the dimensionality of the problem. The dataset produced is stored in `reduced.csv`.\n",
        "\n",
        "We then use this reduced dataset to train a logicistic regression classifier. The code for this is in the file `log_reg.py`. After the training is complete, the code serializes the trained model and stores it in Pickle format in `classifier.pkl`, so it can be shared and reused."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "behind-formation",
      "metadata": {
        "id": "behind-formation"
      },
      "source": [
        "### Representing a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stopped-incident",
      "metadata": {
        "id": "stopped-incident"
      },
      "source": [
        "DVC sees pipelines as collections of steps. Each step has some inputs or **dependencies** and produces **outputs**. The different stages are linked to each other through these.\n",
        "\n",
        "Furthermore, a pipeline can have one or more **parameters** that control its stages and customize their behaviour.\n",
        "\n",
        "DVC stores descriptions of pipelines in a structured file written in the YAML format. Here is how the above example could be represented:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "active-exclusive",
      "metadata": {
        "id": "active-exclusive"
      },
      "source": [
        "```yaml\n",
        "stages:\n",
        "  pca:\n",
        "    cmd: python reduce_dim.py\n",
        "    deps:\n",
        "      - reduce_dim.py\n",
        "      - samples.csv\n",
        "    params:\n",
        "      - total_var\n",
        "    outs:\n",
        "      - reduced.csv \n",
        "  classification:\n",
        "    cmd: python log_reg.py\n",
        "    deps:\n",
        "      - log_reg.py\n",
        "      - reduced.csv\n",
        "    outs:\n",
        "      - classifier.pkl\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "medium-gabriel",
      "metadata": {
        "id": "medium-gabriel"
      },
      "source": [
        "Every step has its own section, appropriately named (`pca`, `classification`). Each section specification has:\n",
        "- the command to run that step (`cmd`)\n",
        "- a list of dependencies (`deps`) which feed into the step\n",
        "- a list of outputs (`outs`) that the step produces\n",
        "- a list of parameters (`params`) that control that step\n",
        "\n",
        "Note that the dependencies include the code itself, as well as any input files!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "considerable-copper",
      "metadata": {
        "id": "considerable-copper"
      },
      "source": [
        "The parameters are stored in a separate file. By default this is called `params.yaml` but other formats are allowed.\n",
        "\n",
        "In this example, the file needs to contain one parameter (the total variance explained by the chosen PCA dimensions):\n",
        "\n",
        "```yaml\n",
        "total_var: 0.9\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dutch-damages",
      "metadata": {
        "id": "dutch-damages"
      },
      "source": [
        "### Creating steps\n",
        "\n",
        "Rather than write the above file all at once, we can run the steps in order and let DVC create the file.\n",
        "\n",
        "To do this, we need to tell it how to execute each step, by using the following command at the terminal:\n",
        "\n",
        "```bash\n",
        "dvc run -n pca \\\n",
        "        -d reduce_dim.py -d samples.csv \\\n",
        "        -p total_var \\\n",
        "        -o reduced.csv \\\n",
        "        python reduce_dim.py\n",
        "```\n",
        "\n",
        "The different aspects are given in the command options:\n",
        "- `-n`: name of the step\n",
        "- `-d`: dependencies\n",
        "- `-p`: parameters\n",
        "- `-o`: outputs\n",
        "- finally, the command to run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "latin-documentation",
      "metadata": {
        "id": "latin-documentation"
      },
      "source": [
        "### Reproducing a pipeline\n",
        "\n",
        "We can run all steps in a pipeline using the command `dvc repro`.\n",
        "\n",
        "Often, running all steps will be redundant. For example, if we have only made changes to the `log_reg.py` file since the last time we ran the pipeline, then the previous step is unaffected and does not need to be rerun. \n",
        "\n",
        "However, if we had updated the samples file or modified the parameter controlling the PCA step, then that step would need to be rerun, as well as all steps downstream of it.\n",
        "\n",
        "By tracking changes to files and analysing the structure of the pipeline, DVC can infer which steps have changed and only run those."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-health",
      "metadata": {
        "id": "ordered-health"
      },
      "source": [
        "## Metrics and outputs\n",
        "\n",
        "DVC also allows us to compare the performance of our models as we make changes to them. We do this by declaring **metrics** as part of a pipeline.\n",
        "\n",
        "In the above example, let's assume we have an additional step which evaluates our trained classifier and stores the results in a file `scores.json`. If we compute two performance metrics, the precision and the area under the ROC curve, the results may look like:\n",
        "\n",
        "```json\n",
        "{ \"precision\": 0.63, \"roc_auc\": 0.85 }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "general-sigma",
      "metadata": {
        "id": "general-sigma"
      },
      "source": [
        "We can record this by expanding the pipeline description with an extra step, like this:\n",
        "```yaml\n",
        "stages:\n",
        "  (...as above...)\n",
        "  evaluation:\n",
        "    cmd: python evaluate.py\n",
        "    deps:\n",
        "      - evaluate.py\n",
        "      - classifier.pkl\n",
        "    metrics:\n",
        "      - scores.json\n",
        "```\n",
        "\n",
        "Running the whole pipeline with `dvc repro` will now also produce the file with the scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "married-headline",
      "metadata": {
        "id": "married-headline"
      },
      "source": [
        "Let's make a change to the parameters file, e.g. to increase `total_var` to 0.95.\n",
        "\n",
        "DVC will notice this and can show us the difference if we run `dvc params diff`:\n",
        "\n",
        "```\n",
        "Path         Param          Old    New\n",
        "params.yaml  total_var      0.9    0.95\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "administrative-template",
      "metadata": {
        "id": "administrative-template"
      },
      "source": [
        "We can also easilly inspect what effect this has on the performance metrics. If we run the pipeline again, then `dvc metrics diff` will show us how the metrics have changed:\n",
        "\n",
        "```\n",
        "Path         Metric     Old     New      Change\n",
        "scores.json  precision  0.63    0.65     0.02\n",
        "scores.json  roc_auc    0.85    0.91     0.06\n",
        "```\n",
        "\n",
        "In two commands, we can run a whole series of steps and inspect the results - in this case, see that changing how we choose features after PCA has improved performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "social-miniature",
      "metadata": {
        "id": "social-miniature"
      },
      "source": [
        "## Other DVC features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brave-advantage",
      "metadata": {
        "id": "brave-advantage"
      },
      "source": [
        "- **Automated plots** for metrics\n",
        "- **Experiments** collect different runs and present them cleanly\n",
        "- **Pushing** to remote storage, for backups and sharing\n",
        "- **Python API** lets us call DVC functionality from a program (e.g. add a new file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-genre",
      "metadata": {
        "id": "indie-genre"
      },
      "source": [
        "## Summary\n",
        "- DVC supports including data and files from other projects, from a variety of sources.\n",
        "- Combining steps in pipelines makes it simpler to rerun analyses without manual intervention.\n",
        "- Apart from files, DVC can also track metrics and automatically present the effect of changes."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Lecture25_DataVersionControl_II.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}